{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GjUMY8dX0vh"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQlbA3XQX0vl"
      },
      "source": [
        "# Generative deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpkgy-aEX0vl"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viIPJYJ6X0vm"
      },
      "source": [
        "### A brief history of generative deep learning for sequence generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPN4YKfLX0vm"
      },
      "source": [
        "### How do you generate sequence data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z0LqatWX0vn"
      },
      "source": [
        "### The importance of the sampling strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPdIv4klX0vn"
      },
      "source": [
        "**Reweighting a probability distribution to a different temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ZVtG_ImX0vn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU3rJ1ptX0vo"
      },
      "source": [
        "### Implementing text generation with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMLxWd0FX0vo"
      },
      "source": [
        "#### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h7ifrYWX0vo"
      },
      "source": [
        "**Downloading and uncompressing the IMDB movie reviews dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CsYtPJlX0vo",
        "outputId": "169ccac5-d8da-4533-bad5-51aff21e4b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-17 04:39:22--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  68.3MB/s    in 1.2s    \n",
            "\n",
            "2022-12-17 04:39:24 (68.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEjrn8ohX0vo"
      },
      "source": [
        "**Creating a dataset from text files (one file = one sample)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY5jVzNTX0vp",
        "outputId": "4e9fbb17-f3b0-45c4-a25d-98802b31db63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb2_0xqyX0vp"
      },
      "source": [
        "**Preparing a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zcOXIe0sX0vp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLhSVH_zX0vp"
      },
      "source": [
        "**Setting up a language modeling dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JOHBLE1IX0vp"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4gt1OqSX0vp"
      },
      "source": [
        "#### A Transformer-based sequence-to-sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gdZEnMf8X0vp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn8ljYsiX0vq"
      },
      "source": [
        "**A simple Transformer-based language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7MZDT2cqX0vq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0WNy34-X0vq"
      },
      "source": [
        "### A text-generation callback with variable-temperature sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31TW9MQpX0vq"
      },
      "source": [
        "**The text-generation callback**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ev8m02GGX0vq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,\n",
        "                 generate_length,\n",
        "                 model_input_length,\n",
        "                 temperatures=(1.,),\n",
        "                 print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            print(\"== Generating with temperature\", temperature)\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(predictions[0, i, :])\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nHYBT-nX0vq"
      },
      "source": [
        "**Fitting the language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkpGvRx_X0vq",
        "outputId": "0e08a765-f847-4fc2-d3c9-d1c8f8d12680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  6/391 [..............................] - ETA: 2:26 - loss: 5.4619"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1823s vs `on_train_batch_end` time: 0.1972s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - ETA: 0s - loss: 5.1714== Generating with temperature 0.2\n",
            "This movie is outstanding made to because last it morning was it it it finds was that true it films almost well something that more the of [UNK] her to life this [UNK] is what [UNK] you wondered see is look that like this [UNK] film should will be certainly based works\n",
            "== Generating with temperature 0.5\n",
            "This movie is is a not great worth movie watching to when watch the it end was of it value was i the gave dvd it a time number from of 1 [UNK] weeks hammer the and beasts i netflix find and it over would my not mouth see i this am\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is stands a another main in story a this little movie less is the better pace than the [UNK] plot is changes convincing from in beginning [UNK] all the 1 animation unfortunately would there do is this an a effort folly with of the character ending tony is gives\n",
            "== Generating with temperature 1.0\n",
            "This movie show is is to a have while been this great storyline the i worst loved movie the but new about character [UNK] barbara it stanwyck was didnt some be good [UNK] as with he very was little going [UNK] to from video him places the and heart [UNK] but with\n",
            "== Generating with temperature 1.5\n",
            "This movie title is has in to such pick a a fictional [UNK] film lie you to become pick one my too emotions dont such get a any minute sense this of was really not good to film add i its was [UNK] the at plot least [UNK] a and few the\n",
            "391/391 [==============================] - 159s 404ms/step - loss: 5.1714\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.7629== Generating with temperature 0.2\n",
            "This movie is is one typically of surprised my actors only who rate arent this cast movie  will i think of the books about the girls who can get there is an impression that the film is that the great one should see this movie prior to this movie after seeing\n",
            "== Generating with temperature 0.5\n",
            "This movie is is a like great someone music whos show god off it the then totally what blew about me the how cover it a was way [UNK] right cast this did is [UNK] a right good where game the previous run star out one a would single be point able\n",
            "== Generating with temperature 0.7\n",
            "This movie [UNK] is the the case acting of is the based cause on of in the my actual tape quality of about a [UNK] movie and and the recognized good once material the is story top clearly notch their and script this and movie the is [UNK] too of much the\n",
            "== Generating with temperature 1.0\n",
            "This movie film works is more so [UNK] much me of in the bette past midler pirate gives prisoner the [UNK] crowd performance [UNK] ron in [UNK] a voice liberal [UNK] and [UNK] furious are this found is other basically films an that attractive its [UNK] as kelly back is and a\n",
            "== Generating with temperature 1.5\n",
            "This movie is is garbage a i hollywood very disney high hollywood school is and a at roller high coaster male and quite not the enough number the of [UNK] children [UNK] for on 25 vacation years jim but toy as show their and [UNK] they follow are them killers a cowritten\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 4.7629\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.5681== Generating with temperature 0.2\n",
            "This movie must was have by been far held too hostage much the better plot person is than from having their other marriage wacky goons kid being stay [UNK] away penguins he it loves has him nothing a spectacular whole rock lot n and roll also night the on smartest all lifestyle\n",
            "== Generating with temperature 0.5\n",
            "This movie is is not truly correct a the comedy movie central and sharp warning enough contains visual an effects insult which to beth entertainment hughes industry funny loved actors was an facing angel class in man london while kate [UNK] kirk [UNK] douglas insist and medicine [UNK] man on per the\n",
            "== Generating with temperature 0.7\n",
            "This movie small is television [UNK] in with manhattan extremely family cheesy who little steal or a the month other of characters britain wander namely replacing [UNK] others bland [UNK] bikers rebellious someone [UNK] accidentally no destroy better metal yet homages shes him the only subject south of american danger society in\n",
            "== Generating with temperature 1.0\n",
            "This movie might is be a a nostalgic movie movie one for i a liked while few it times would in not [UNK] be [UNK] unflinching debra as [UNK] the cup characterizations of are tea enormous who amounts has to not a let decent alone translation can it be be lacking seen\n",
            "== Generating with temperature 1.5\n",
            "This movie movie is should so get missing full it moon is so still awful on the imdbs bandwagon bottom rating 100 in this say is its not obvious [UNK] that which it make does the represent experiments the they original explain information why of they the have plot attacked or by\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 4.5681\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.4511== Generating with temperature 0.2\n",
            "This movie time was really i made thought a that lot young i viewers like were id trying never to heard see of but titanic having before checked had it read and 3 incomplete minutes uwe of boll my who life said with however rock upon n this roll disgusted music me\n",
            "== Generating with temperature 0.5\n",
            "This movie movie about shows a a [UNK] [UNK] who victor saw [UNK] hit a a new hitting life a and war cannot several revive traditional the indian events culture that of would human later magic appeal then to success i a also bit helped of to [UNK] find the one [UNK]\n",
            "== Generating with temperature 0.7\n",
            "This movie film was is [UNK] a [UNK] gem which not was only ever one released could from see not or very [UNK] entertaining the it story isnt tells trying the too true much to given be me a im blank not [UNK] do to not give da you a a [UNK]\n",
            "== Generating with temperature 1.0\n",
            "This movie film was was terrible meant it to was be a sure comedy there but were this many was a the vehicle authentic that portrayal [UNK] of was john really payne nothing and that even is slightly as odd bad as as its the a [UNK] waste at of the the\n",
            "== Generating with temperature 1.5\n",
            "This movie isnt is that cheesy it and has makes some it of impossible the to sequels grant of works both are the believable acting in is this this man [UNK] just job isnt at one least point fun the is one that of is the being prisoners a of deep love\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 4.4511\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.3689== Generating with temperature 0.2\n",
            "This movie one is had a some great good chemistry plot between and the ellen rest de walk haunted the house movie give takes me place a several responsibility times for filled the to movie have winds its in final wellwritten chase and with then real its action a uma nice thurman\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is was homage absolutely to fantastic the on actors comedy and genre comedy it of was speaking very with much major comedy spark like of hogan theres a something reason about for this being movie a is failed not out funny of except any for substance the where ride\n",
            "== Generating with temperature 0.7\n",
            "This movie is was almost so as flipping mass through plane tv site all so it i was checked as out to on write four about consists five of of people [UNK] say for that their this followers movie did was nothing so for cliched the already first bad minute movie i\n",
            "== Generating with temperature 1.0\n",
            "This movie was has made nothing a better strong effect cast than and usual that there in seemed this to lame rule plot movies point on that fault cover it i never forgot became about aware industrial that [UNK] i dreadful turning ghastly it acting off in so the much two i\n",
            "== Generating with temperature 1.5\n",
            "This movie is contains by real far events the going story forth liotta between and a a real guy person the after shots helping the of way a through team this rushing seemed back all to animation our a nuns soul problem plane was would that really age stole full the moon\n",
            "391/391 [==============================] - 159s 405ms/step - loss: 4.3689\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.3050== Generating with temperature 0.2\n",
            "This movie was normally truly the horrible best title movie in i the think original thats was right made there it are awful some but movies it only has doesnt a look hint good of such struggle taboo or but deep with south no [UNK] plot we its are supposed in to\n",
            "== Generating with temperature 0.5\n",
            "This movie is [UNK] rubbish to i avoid loved reading it the because dvd the was curse one of the my worst least movies favorite of of all the time bmovie the movies train ive detectives ever firm seen estate done has by the the very best best movie its [UNK] very\n",
            "== Generating with temperature 0.7\n",
            "This movie is is amazing an is amazing reminiscent thing of about adam this sandler theater hollywood comedy actually is made somehow for very each very and unique not although a it very reminds weird me that little he man discovered to be be the [UNK] main home character of is a\n",
            "== Generating with temperature 1.0\n",
            "This movie escaped is from not real a life big but terrorists certainly turn does out not the really same bad type but of it course just is seems no that where people to are feel and very seeks unlikely to to buy survive a under really the weird chance relationship to\n",
            "== Generating with temperature 1.5\n",
            "This movie might has be the the worst best thing of to all say the ever pain seen of and the the actors fight were between well the done two i scenes hated an the [UNK] first scene movie was made so this boring makes in it a i bad very scene\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 4.3050\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.2538== Generating with temperature 0.2\n",
            "This movie film is was really shot painful two and stranded many on nubile earth daughters not or many sunday of is the kind least the bearable english word dubbing no isnt im a a mere huge presence fan at and least very his pleasing katherine shots jill its [UNK] out werewolf\n",
            "== Generating with temperature 0.5\n",
            "This movie was frances risqué upper but class otto movie one they would even play never the seen actors before would but wish be they a know macbeth in had the a success [UNK] on accent tv the were entire so aamir long edited and i [UNK] actually have hated read it\n",
            "== Generating with temperature 0.7\n",
            "This movie is is ruthless right unpretentious around and the scarcely world endearing after from bad the acting big beware hate this it movie the the movie film did has try no and classics they like really for kubrick a to film cause but south of of course this dont is need\n",
            "== Generating with temperature 1.0\n",
            "This movie some could of be possibly say the it best ranks sucked in a it traditional ends humor up and asking falling for down gods yes dear there readers is is no some entertainment prolonged value wanted in i this hated extremely [UNK] poor the writing acting sorry of very bad\n",
            "== Generating with temperature 1.5\n",
            "This movie film is plays a on middleaged board tour cutting with all a accounts eyes only at a the few look stalin of landscapes a and [UNK] feast ned the kelly russians footage where was guards shot arguing and about lees modern passions politics [UNK] eg wild deeply al i [UNK]\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 4.2538\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.2111== Generating with temperature 0.2\n",
            "This movie is is good a at very times attract but car with depends the on tolerance theres i a and bit i where can a one ripoff as of the the better pulp being fiction said plus without a the [UNK] cult its classic marlene  dietrich tried to put its\n",
            "== Generating with temperature 0.5\n",
            "This movie is tells it the is first fantastic that it i never havent would seen think a so nude for and yet there i is could this possibly movie be but the they worst people movie [UNK] ever [UNK] camera most my off time the the movie second sex half scene\n",
            "== Generating with temperature 0.7\n",
            "This movie is is very stupid [UNK] but spoil as it for is you the two main sides characters but friendship theyre does not everything run everything and goes story to is the complete bollywood nonsense story but is finds to love be and peril has an some intriguing kind plot of\n",
            "== Generating with temperature 1.0\n",
            "This movie is was the just worst like movie productions first i of would all have say southern youd accents expect but for you me the the biggest lady problem was was she with was racist actually cinema i but was when like my her dad character was was white going this\n",
            "== Generating with temperature 1.5\n",
            "This movie is is about well the known first stranded saw on and film cold version london of was this it movie was surprised a its time a the real ship story had is an totally accident different i the can supernatural remember was being was [UNK] built in in the the\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 4.2111\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.1747== Generating with temperature 0.2\n",
            "This movie excess was of one a of small the pleasures actions of to wealthy be father sure a the personal history criminal of is the not story only but in for hollywood this for is years an that entertaining brings reason us for believing everyone an else even who if cares\n",
            "== Generating with temperature 0.5\n",
            "This movie collection is of errol four [UNK] talented for stars a in career madman he al has pacino helmed has a run wellwritten the and screenplay preach by to his spread films his inspired reputation talent seems as to though do he superman has new dated fight 16 scenes years and\n",
            "== Generating with temperature 0.7\n",
            "This movie game was blows fabulous one the of resulting the sting subject built of in arrangement the the latter game and the the fight cover between does the what game kind is of they sit all their the way way through through while their this deal tasks with the drug knives\n",
            "== Generating with temperature 1.0\n",
            "This movie is features just bad as bad advertised as for the [UNK] fact fan ouch story no line real good idea acting to quite bad gym acting shiny from boy despite to the horrible fact directing for not the so top bad that i i will didnt not expect even that\n",
            "== Generating with temperature 1.5\n",
            "This movie is is not rather will lame not the at only least slow bit time the did first not but bother i me did i i was would hoping not it to would be stay to fresh shoot but em its smile the theres addition catherine of zetajones bad and acting\n",
            "391/391 [==============================] - 159s 405ms/step - loss: 4.1747\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.1430== Generating with temperature 0.2\n",
            "This movie is is great about it christmas the present acting day is life a she huge is fear perfection her around acting her in love a shop movie strange this things is begin also to too torture here people and hate place me like mad a but woman this is movie\n",
            "== Generating with temperature 0.5\n",
            "This movie movie has was the a [UNK] little about story sex that and [UNK] that the can older make people a go movie out without of having the a mental child illness reminds story me based of on a my teenage friends years and ago it but was all really really\n",
            "== Generating with temperature 0.7\n",
            "This movie film has is better actually than it the has humor a and [UNK] therefore feel it it is was [UNK] a by whole far art hollywoods cinema best [UNK] in deserves the it ten is out my of only 10 seven [UNK] films the where acting [UNK] [UNK] and is\n",
            "== Generating with temperature 1.0\n",
            "This movie is [UNK] not is a the performance usual of the a guy school who sex has all [UNK] the and religion when and its his quite closest brilliant thing in to europe man is what not we in look hollywood back the on 20th a century hundred class [UNK] members\n",
            "== Generating with temperature 1.5\n",
            "This movie film stands seems as to anyone how concerned [UNK] when pass either by faster the or strings more depending [UNK] on keeps your your attention head from many your [UNK] own of parallel surrealism world rolling hippies your and mist youre ate becoming your too bergman squeamish and i it\n",
            "391/391 [==============================] - 159s 405ms/step - loss: 4.1430\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb265949700>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.fit(lm_dataset, epochs=10, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkhTBoeGX0vr"
      },
      "source": [
        "### Wrapping up"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}